<!DOCTYPE html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">

    <!-- Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>

    <!-- Styles -->
    <link rel="stylesheet" href="/QDeepEducation/style/general.css">
    <link rel="stylesheet" href="/QDeepEducation/style/desktop.css">
    <link rel="stylesheet" href="/QDeepEducation/style/mobile.css">

    <!-- Scripts -->
    <script src="/QDeepEducation/js/header.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <script src="/QDeepEducation/js/mathjax.js"></script>
</head>
<body>
    <div class="header">
        <div class="logo">
            <a class="logo_link" href="/QDeepEducation/index.html"><img src="/QDeepEducation/source/qdeep_edu.png"></a>
        </div>
    </div>
    <div class="learn_page">
        <!-- Сделать отдельный sidebar для мобильной версии TODO -->
        <div class="sidebar">
            <a href="/QDeepEducation/html/sem1.html"><h2>Квантовая информация</h2></a>
            <a href="#"><h2>Кубит</h2></a>
        </div>
        <div class="content">
			<h1>Учебные семинары</h1>
			    Черновая версия конспектов материалов учебных семинаров лаборатории квантовых вычислений.
			<h2>Введение в квантовую информацию</h2>
			<h3>Введение</h3>
			Лаборатория квантовых вычислений работает с программным обеспечением в области квантовых технологий. Наша задача заключается в том, чтобы соединить научный мир квантовых вычислений с обычным потребителем. Для повышения качества разработки начали свою работу учебные семинары. Основная технология научных семинаров в настоящий момент&mdash; это технология квантового отжига. Квантовый отжиг довольно сложная технология для понимания которой нужно изучить различные дисциплины. А именно
			<ul>
			    <li>Квантовая механика.</li>
			    <li>Квантовые вычисления.</li>
			</ul>
			Для комфортного владения квантовой механикой и квантовых вычисленей необходимо знание математического аппарата. А именно
			<ul>
			    <li> Линейная алгебра.</li>
			    <li> Математический анализ.</li>
			    <li> Теория вероятностей.</li>
			    <li> Дифференциальные уравнения.</li>
			</ul>
			<h3>Классическая информация</h3>
			Начнём наш первый семинар с введения в квантовую информацию. Но чтобы понимать принципы работы квантовой информации, необходимо понимать принципы работы классической информации. Центральным объектом изучения классической информации является бит. Слово &laquo;<i>бит</i>&raquo; произошло от английского слова &laquo;bit&raquo;, которое расшифровывается как <b>bi</b>nary digi<b>t</b>. В настоящее время слово бит является уже вполне привычным и очевидным. Но это ранее не было так очевидно. 
			Всё началось с задачи передачи информации. Пусть Алиса хочет передать сообщение Бобу. Сообщение &mdash; это некоторая последовательность символов некоторого языка. Пусть $M$ &mdash; это количество символов алфавита некоторого языка. И пусть $N$ &mdash; это длина последовательности. В 1928 году Ральф Хартли предложил свой способ измерения информации \cite{hartley1928transmission}, которая в настоящее время известна как формула Хартли.
			\begin{equation*}
			    I = N \cdot \log_2 M,
			\end{equation*}
			где $I$ &mdash; это <i>количество информации</i>, а величина $H = \log_2 M$ была названа <i>информационной энтропией</i>. 
			
			<div class="history">
				<div><img src="/QDeepEducation/source/person/hartley.jpg"></div>
				<div>
					<b>Ральф Винтон Лайон Хартли</b> (30 ноября 1888 &ndash; 1 мая 1970) &mdash; американский исследователь в области электротехники. В 1928 году предложил свой способ измерения количества информации в передаваемом сообщении в работе &laquo;Transmission of information&raquo; в журнале &laquo;Bell System technical journal&raquo;. Родился в городе Спрусмонт, штат Невада. В 1909 году окончил университет Юты. В 1913 закончил Оксфордский университет. В 1915 году руководил разработкой радиоприемника для испытаний трансатлантической радиотелефонной системы Bell System.
				</div>
			</div>
			    
			Здесь важно прокомментировать такой способ измерения информации. Пусть у нас есть два символа $a, b$. Их можно закодировать с помощью нуля и единицы: $a \mapsto 0, \ b \mapsto 1$. Пусть теперь у нас есть три символа $a, b$ и $c$. Закодировать нулём и единицей их мы уже не сможем. Здесь будет нужна последовательность из двух нулей/единиц. Например,
			\begin{equation*}
			    a \mapsto 00, \ b \mapsto 01, \ c \mapsto 10.
			\end{equation*}
			Для 4 символов также хватит последовательности длины 2. Но для 5 символов уже нужна будет последовательность длины 3.
			\begin{equation*}
			    a \mapsto 000, \ b \mapsto 001, \ c \mapsto 010, \ d \mapsto 011, \ e \mapsto 100.
			\end{equation*}
			На этом этапе может возникнуть вопрос: почему мы кодируем только нулём и единицей? На самом деле, кодировать можно и большим количеством цифр. Но удобнее всего использовать две цифры. 
			Исходя из представленных примеров можно сделать вывод, что $M \leqslant 2^n$, где $n$ &mdash; минимальное количество цифр в последовательности, удовлетворяющее неравенству. Отсюда несложно получить выражение для $n$:
			\begin{equation*}
			    n = \left\lceil \log_2 M \right\rceil,
			\end{equation*}
			где $\left\lceil x \right\rceil$ &mdash; это операция округления вверх. Важно отметить, что Хартли ещё не использовал термин бит, хотя уже ввёл двоичную систему измерения информации.
			У формулы Хартли был важный недостаток. Его формула не учитывала шумы и внешнее воздействие на канал передачи информации. Спустя два десятилетия Клод Шеннон предложил несколько иной способ измерения информации \cite{shannon1948mathematical}. Пусть теперь у каждого символа алфавита есть вероятность появления $p_i$ при получении сообщения. Тогда информационная энтропия по Шеннону примет вид
			\begin{equation*}
			    H = -\sum_{i = 1}^M p_i\log p_i.
			\end{equation*}
			Оказывается, формула Хартли является частным случаем формулы Шеннона. Докажем это утверждение.

			<div class="proof">
				Пусть вероятности появления символов одинаковые. То есть $\forall i \ 1 \leqslant i \leqslant M \ p_i = 1/M$. Подставим эти вероятности в формулу Шеннона.
			    <div class="math">
					\begin{align*}
						H = -\sum_{i = 1}^M p_i\log p_i = -\sum_{i = 1}^M \frac{1}{M}\log \frac{1}{M} = -M\cdot\frac{1}{M}\log \frac{1}{M} = \log M.
					\end{align*}
				</div>
			    <div class="qed">&#9723;</div>
			</div>

			<div class="history">
				<div><img src="/QDeepEducation/source/person/shennon.jpg"></div>
				<div>
					<b>Клод Элвуд Шеннон</b> (30 апреля 1916 &ndash; 24 февраля 2001)&mdash; американский инженер и математик. В 1948 году в работе &laquo;A mathematical theory of communication&raquo; ввёл обобщенное понятие информационной энтропии. Начал использовать термин &laquo;бит&raquo; в научной литературе. Родился в городе Петоски, штат Мичиган, США. В 1936 году окончил Мичиганский университет. 
			История термина &laquo;бит&raquo; довольно примечательна. Его начал использовать Клод Шеннон в работе, в которой он ввёл обобщенную информационную энтропию. Причём в этой работе он пишет: &laquo;The choice of a logarithmic base corresponds to the choice of a unit for measuring information. If the
			base 2 is used the resulting units may be called binary digits, or more briefly bits, a word suggested by
			J. W. Tukey&raquo; \cite{shannon1948mathematical}. Таким образом, термин &laquo;бит&raquo; впервые использовал Джон Тьюки.
				</div>
			</div>

			<div class="history">
				<div><img src="/QDeepEducation/source/person/Tukey.jpg"></div>
				<div>
					<b>Джон Уайлдер Тьюки</b> (16 июня 1915 &ndash; 26 июля 2000) &mdash; американский математик. Автор термина &laquo;бит&raquo;. Родился в городе Нью-Бедфорд, штат Массачусетс, США. Высшее образование получил в Брауновском университете. Докторскую степень получил в Принстонском университете. Во время Второй мировой войны работал в Bell Labs.
				</div>
			</div>
			    
			<h3>Физические основы информации</h3>
			Информация нам интересна с точки зрения вычислений. Поэтому важно понять, каким образом происходят классические вычисления. А для этого нужно изучить физические основы кодирования классической информации. Самый распространённый способ кодирования информации&mdash; это электричество. Интуитивный способ кодирования: 0 $\mapsto$ тока нет, 1 $\mapsto$ ток есть. В начале XX века был изобретён <i>триод</i>. 
			
			<div class="figure"><img width="250px" src="/QDeepEducation/source/img/triode.png"></div>
			
			Триод представляет из себя газоразрядную лампу, в которой ток проходит от катода к анода. А напряжение на сетке позволяет контролировать силу тока между анодом и катодом. Если выставить некоторую границу для силы тока, то можно получить закодированное бинарное значение. То есть, если сила тока меньше порогового значения, то кодируется ноль; если же сила тока больше порогового значения, то кодируется единица. С помощью подобных газоразрядных ламп и были созданы первые ЭВМ. Главный недостаток такого подхода заключался в сложности масштабирования. Для работы требовалось тысячи ламп. Но если сломаются хотя бы несколько из них, вероятность чего достаточно велика, то вся система перестанет работать. 
			Ситуация радикально изменилась после полупроводниковой революции. В конце 50-60-х годов были открыты материалы полупроводники. С помощью полупроводников удалось сделать устройство, по назначению схожее с триодом&mdash; транзистор. Во-первых, эти устройства были на порядки компактнее. Во-вторых, они были более надёжными, чем газоразрядные лампы. Таким образом, изобретение полупроводников позволило масштабировать объёмы информации, которые способны обрабатывать ЭВМ.
			
			<div class="figure"><img width="450px" src="/QDeepEducation/source/img/trans.jpg"></div>

			Важно отметить, что выше был описан только способ кодирования информации. На практике в настоящее время транзисторы используются в качестве вентилей. Способы постоянного хранения информации отличаются от описанного.
			<h3>Квантовые системы</h3>
			В XX веке исследователи думали об альтернативных способах кодирования информации. В частности была гипотеза, что возможно использовать квантовые системы  для кодирования информации. Примеры квантовых систем: поляризация фотона, спин электрона, энергетические уровни иона и так далее.
			В квантовых системах способы кодирования появляются естественным образом, о чём мы поговорим чуть позже. Сейчас важно обсудить некоторые свойства квантовых систем. Например, <i>корпускулярно-волновой дуализм</i>. Дуализм (то есть двойственность) заключаются в том, что квантовая система может обладать свойствами как волны, так и частицы (корпускулы). В качестве примера волнового свойства можно рассмотреть интерференцию. Пусть есть два источника гармонических колебаний. Тогда волны от двух источников складываются и появляются области, в которых волны достигают своего максимума и минимума.
			    
			<div class="figure"><img width="500px" src="/QDeepEducation/source/img/ducks.jpg"></div>

			В качестве проявления свойства частицы достаточно рассмотреть частицу, как очень маленькое тело. То есть оно обладает массой, импульсом. Также частица может оставить точечный след на некоторой поверхности. Важнейшим опытом, доказывающим наличие корпускулярно-волнового дуализма был двухщелевой опыт Юнга. Изначально этот опыт был продемонстрирован на свете. Но сейчас нам будет интересен этот эксперимент на электронах. С одной стороны, электрон является частицей. В этом сомненья нет. Но вот как понять, что электрон является волной? 
			    
			<div class="figure"><img width="500px" src="/QDeepEducation/source/img/ng.jpg"></div>

			Схема установки следующая. Есть два экрана. На одном из экраном имеются две прорези. Слева от экрана есть источник электронов. Электроны вылетают из источника, проходят через щели и попадают на второй экран. Если бы электрон был классической частицей, то мы бы увидели только две полоски. Но на самом деле мы видим целую череду полосок. Эта череда полос и есть интерференционная картина. то есть фиксированные области максимумов и минимумов попадания электронов. Таким образом, электрон проявляет как свойства частицы, так и волны. 
			
			<div class="history">
				<div><img src="/QDeepEducation/source/person/yuong.jpg"></div>
				<div>
					<b>Томас Юнг</b> (13 июня 1773 &ndash; 10 мая 1829) &mdash; английский физик, механик, астроном и филолог. Родился в городе	Милвертон, Англия. В конце XVIII &ndash; начале XIX веков занимался изучением волновой природы света. В 1803 году опубликовал работу, в которой описал двухщелевой опыт со светом.
				</div>
			</div>

			Помимо корпускулярно-волнового дуализма квантовые системы обладают такими свойствами как <i>суперпозиция</i> и <i>запутанность</i>. Таким образом, квантовые системы обладают различными свойствами, которыми не обладают классические системы. Здесь может возникнуть закономерный вопрос. Есть ли у квантовых систем преимущество с точки зрения кодирования информации? Ответ: да, имеют.
			<h3>Квантовая информация</h3>
			Пусть необходимо закодировать классическую информацию (то есть обычные биты) в какой-нибудь квантовой системе. Какое количество информации мы сможем закодировать? Квантовые системы дают некоторое преимущество перед классическими, но не безграничное. Именно этот результат получил советский математик А. С. Холево \cite{holevo}. Таким образом, можно сказать, что было получено количественное соотношение между классической и квантовой информацией.
			
			<div class="history">
				<div><img src="/QDeepEducation/source/person/holevo.jpg"></div>
				<div>
					<b>Александр Семёнович Холево</b> (род. 2 сентября 1943) &mdash; советский математик, юниор квантовой информации. Определил верхнюю границу классической информации, которую можно передать в квантовом канале. В настоящее время работает в Математическом институте им. В.А. Стеклова РАН. Читает курс &laquo;Квантовая информация&raquo;.
				</div>
			</div>

			Работа Холево дала толчок к изучению квантовой информации. Обратите внимание, что теория классической информации зародилась из задачи коммуникации. Теория квантовой информации берёт своё начало также из задачи коммуникации. В 1984 году впервые был разработан протокол квантовой коммуникации. Мы не будем его детально обсуждать сейчас. Но мы обсудим ключевые идеи оттуда.
			Мы до сих пор не обсудили как всё-таки можно кодировать информацию с помощью квантовой системы. В качестве физической системы возьмём <i>фотон</i>. Фотону также характерен корпускулярно-волновой дуализм. Поэтому фотон мы можем легко рассматривать как волну. Это является корректным допущением, поскольку свет имеет электромагнитную природу. Электромагнитная волна представляет из себя колебания векторов напряженности электрического и магнитного полей.
			    
			<div class="figure"><img width="500px" src="/QDeepEducation/source/img/wave.png"></div>

			В общем виде волна может быть произвольно ориентирована в пространстве. В частном случае, она может колебаться только в горизонтальной или вертикальной плоскостях. Это называется <i>поляризацией</i> электромагнитной волны. Введём обозначения. Пусть $\ket{\leftrightarrow}$ &mdash; горизонтальная поляризация,  $\ket{\updownarrow}$ &mdash; вертикальная поляризация. Суть обозначения $\ket{\ \cdot\ }$ мы поясним позже. Идея кодирования следующая: $0 \mapsto \ket{\leftrightarrow}$ и $1 \mapsto \ket{\updownarrow}$. Но помимо горизонтальной и вертикальной поляризации могут быть и промежуточные. Это можно описать с помощью комбинации
			\begin{equation*}
			    \alpha\ket{\leftrightarrow} + \beta\ket{\updownarrow}.
			\end{equation*}
			Суть коэффициентов $\alpha$ и $\beta$ мы обсудим на следующем занятии. Но даже сейчас видно, что квантовая система может описать очень много разных <i>состояний</i>. Причём бесконечно много. В следующий раз мы обсудим математический формализм квантовой информации.
        </div>
    </div>
</body>